------Training ApprxReinforcementAgent------
Training on 100000 epochs.
Averages were 6.77432 epochs and -1.00696 rewards.===========] 100.0% ...,99999
Finished training.

------Testing ApprxReinforcementAgent------


Test 1 with starting state 790000000000011101000010004700011110110001010100002111600000000000:
000000000
00   0 00
00 000 00
0    0  0
00 0 0 00
00r   h00
000000000

--starting state--
000000000
00   0 00
00 000 00
0    0  0
00 0 0 00
00r  h 00
000000000

step  : 1
reward: -1

000000000
00   0 00
00 000 00
0    0  0
00 0 0 00
00r h  00
000000000

step  : 2
reward: -2

000000000
00   0 00
00 000 00
0    0  0
00 0 0 00
00rh   00
000000000

step  : 3
reward: -3

000000000
00   0 00
00 000 00
0    0  0
00 0 0 00
00     00
000000000

step  : 4
reward: 2

Testing Summary:
  Agent took 4 steps and earned a reward of 2.


Test 2 with starting state 79000000000007110100001000100014210110001010600001111100000000000:
000000000
00h  0 00
00 000 00
0 sr 0  0
00 0 0h00
00     00
000000000

--starting state--
000000000
00h  0 00
00 000 00
0 sr 0  0
00 0 0 00
00    h00
000000000

step  : 1
reward: -1

000000000
00h  0 00
00 000 00
0 sr 0  0
00 0 0 00
00   h 00
000000000

step  : 2
reward: -2

000000000
00h  0 00
00 000 00
0 sr 0  0
00 0 0 00
00  h  00
000000000

step  : 3
reward: -3

000000000
00h  0 00
00 000 00
0 sr 0  0
00 0h0 00
00     00
000000000

step  : 4
reward: -4

000000000
00h  0 00
00 000 00
0 srh0  0
00 0 0 00
00     00
000000000

step  : 5
reward: -5

000000000
00h  0 00
00 000 00
0 s  0  0
00 0 0 00
00     00
000000000

step  : 6
reward: 0

Testing Summary:
  Agent took 6 steps and earned a reward of 0.


Test 3 with starting state 79000000000001140100006000100011210110007010100001111100000000000:
000000000
00  s0 00
00h000 00
0  r 0  0
00h0 0 00
00     00
000000000

--starting state--
000000000
00  s0 00
00 000 00
0 hr 0  0
00h0 0 00
00     00
000000000

step  : 1
reward: -1

000000000
00  s0 00
00 000 00
0    0  0
00h0 0 00
00     00
000000000

step  : 2
reward: 4

Testing Summary:
  Agent took 2 steps and earned a reward of 4.


Test 4 with starting state 79000000000001110100001000100021110670001010100004111100000000000:
000000000
00   0 00
00 000 00
0r   0hh0
00 0 0 00
00s    00
000000000

--starting state--
000000000
00   0 00
00 000 00
0r   0 h0
00 0 0h00
00s    00
000000000

step  : 1
reward: -1

000000000
00   0 00
00 000 00
0r   0 h0
00 0 0 00
00s   h00
000000000

step  : 2
reward: -2

000000000
00   0 00
00 000 00
0r   0 h0
00 0 0 00
00s  h 00
000000000

step  : 3
reward: -3

000000000
00   0 00
00 000 00
0r   0 h0
00 0 0 00
00s h  00
000000000

step  : 4
reward: -4

000000000
00   0 00
00 000 00
0r   0 h0
00 0h0 00
00s    00
000000000

step  : 5
reward: -5

000000000
00   0 00
00 000 00
0r  h0 h0
00 0 0 00
00s    00
000000000

step  : 6
reward: -6

000000000
00   0 00
00 000 00
0r h 0 h0
00 0 0 00
00s    00
000000000

step  : 7
reward: -7

000000000
00   0 00
00 000 00
0rh  0 h0
00 0 0 00
00s    00
000000000

step  : 8
reward: -8

000000000
00   0 00
00 000 00
0    0 h0
00 0 0 00
00s    00
000000000

step  : 9
reward: -3

Testing Summary:
  Agent took 9 steps and earned a reward of -3.


Test 5 with starting state 79000000000001160100001000700011110210001010100001411100000000000:
000000000
00  h0 00
00 000h00
0    0r 0
00 0 0 00
00 s   00
000000000

--starting state--
000000000
00 h 0 00
00 000h00
0    0r 0
00 0 0 00
00 s   00
000000000

step  : 1
reward: -1

000000000
00h  0 00
00 000h00
0    0r 0
00 0 0 00
00 s   00
000000000

step  : 2
reward: -2

000000000
00   0 00
00h000h00
0    0r 0
00 0 0 00
00 s   00
000000000

step  : 3
reward: -3

000000000
00   0 00
00 000h00
0 h  0r 0
00 0 0 00
00 s   00
000000000

step  : 4
reward: -4

000000000
00   0 00
00 000h00
0  h 0r 0
00 0 0 00
00 s   00
000000000

step  : 5
reward: -5

000000000
00   0 00
00 000h00
0   h0r 0
00 0 0 00
00 s   00
000000000

step  : 6
reward: -6

000000000
00   0 00
00 000h00
0    0r 0
00 0h0 00
00 s   00
000000000

step  : 7
reward: -7

000000000
00   0 00
00 000h00
0    0r 0
00 0 0 00
00 sh  00
000000000

step  : 8
reward: -8

000000000
00   0 00
00 000h00
0    0r 0
00 0 0 00
00 s h 00
000000000

step  : 9
reward: -9

000000000
00   0 00
00 000h00
0    0r 0
00 0 0 00
00 s  h00
000000000

step  : 10
reward: -10

000000000
00   0 00
00 000h00
0    0r 0
00 0 0h00
00 s   00
000000000

step  : 11
reward: -11

000000000
00   0 00
00 000h00
0    0  0
00 0 0 00
00 s   00
000000000

step  : 12
reward: -6

Testing Summary:
  Agent took 12 steps and earned a reward of -6.


Test 6 with starting state 79000000000001110100001000100061110110001010100002147100000000000:
000000000
00   0 00
00 000 00
0h   0  0
00 0 0 00
00r sh 00
000000000

--starting state--
000000000
00   0 00
00 000 00
0 h  0  0
00 0 0 00
00r sh 00
000000000

step  : 1
reward: -1

000000000
00   0 00
00 000 00
0    0  0
00h0 0 00
00r sh 00
000000000

step  : 2
reward: -2

000000000
00   0 00
00 000 00
0    0  0
00 0 0 00
00  sh 00
000000000

step  : 3
reward: 3

Testing Summary:
  Agent took 3 steps and earned a reward of 3.


Test 7 with starting state 790000000000017101000010001000611101100010101000011112400000000000:
000000000
00 h 0 00
00 000 00
0h   0  0
00 0 0 00
00     00
000000000

--starting state--
000000000
00 h 0 00
00 000 00
0 h  0  0
00 0 0 00
00     00
000000000

step  : 1
reward: -1

000000000
00 h 0 00
00 000 00
0    0  0
00h0 0 00
00     00
000000000

step  : 2
reward: -2

000000000
00 h 0 00
00 000 00
0    0  0
00 0 0 00
00h    00
000000000

step  : 3
reward: -3

000000000
00 h 0 00
00 000 00
0    0  0
00 0 0 00
00 h   00
000000000

step  : 4
reward: -4

000000000
00 h 0 00
00 000 00
0    0  0
00 0 0 00
00  h  00
000000000

step  : 5
reward: -5

000000000
00 h 0 00
00 000 00
0    0  0
00 0 0 00
00   h 00
000000000

step  : 6
reward: -6

000000000
00 h 0 00
00 000 00
0    0  0
00 0 0 00
00     00
000000000

step  : 7
reward: -1

Testing Summary:
  Agent took 7 steps and earned a reward of -1.


Test 8 with starting state 79000000000001110700001000100011110110001020400001116100000000000:
000000000
00   0h00
00 000 00
0    0  0
00 0r0s00
00   h 00
000000000

--starting state--
000000000
00   0h00
00 000 00
0    0  0
00 0r0s00
00  h  00
000000000

step  : 1
reward: -1

000000000
00   0h00
00 000 00
0    0  0
00 0 0s00
00     00
000000000

step  : 2
reward: 4

Testing Summary:
  Agent took 2 steps and earned a reward of 4.


Test 9 with starting state 79000000000001110100001000100016110110001070100002411100000000000:
000000000
00   0 00
00 000 00
0 h  0  0
00 0h0 00
00rs   00
000000000

--starting state--
000000000
00   0 00
00 000 00
0    0  0
00h0h0 00
00rs   00
000000000

step  : 1
reward: -1

000000000
00   0 00
00 000 00
0    0  0
00 0h0 00
00 s   00
000000000

step  : 2
reward: 4

Testing Summary:
  Agent took 2 steps and earned a reward of 4.


Test 10 with starting state 790000000000011401000010001000111201100067010100001111100000000000:
000000000
00  s0 00
00 000 00
0   r0  0
00 0 0 00
00     00
000000000

--starting state--
000000000
00  s0 00
00 000 00
0 h r0  0
00h0 0 00
00     00
000000000

step  : 1
reward: -1

000000000
00  s0 00
00 000 00
0  hr0  0
00h0 0 00
00     00
000000000

step  : 2
reward: -2

000000000
00  s0 00
00 000 00
0    0  0
00h0 0 00
00     00
000000000

step  : 3
reward: 3

Testing Summary:
  Agent took 3 steps and earned a reward of 3.
Training Metrics:
+---------+--------+---------+
| episode | epochs | rewards |
+---------+--------+---------+
|    1    |   31   |   -31   |
|  10001  |   4    |    2    |
|  20001  |   8    |    -2   |
|  30001  |   5    |    1    |
|  40001  |   4    |    2    |
|  50001  |   2    |    4    |
|  60001  |   3    |    3    |
|  70001  |   3    |    3    |
|  80001  |   11   |    -5   |
|  90001  |   10   |    -4   |
+---------+--------+---------+
Testing Metrics:
+------+--------+---------+
| test | epochs | rewards |
+------+--------+---------+
|  1   |   4    |    2    |
|  2   |   6    |    0    |
|  3   |   2    |    4    |
|  4   |   9    |    -3   |
|  5   |   12   |    -6   |
|  6   |   3    |    3    |
|  7   |   7    |    -1   |
|  8   |   2    |    4    |
|  9   |   2    |    4    |
|  10  |   3    |    3    |
+------+--------+---------+
Metric Averages:
+---------+---------+
|  Metric | Average |
+---------+---------+
|  epochs |   5.0   |
| rewards |   1.0   |
+---------+---------+
Saving metrics as 'metrics/metric-3'
Weights Table
+---------------+---------------------+
|    Feature    |        Weight       |
+---------------+---------------------+
|      bias     | -19.577750533437747 |
| h1-dist-to-s1 | -1.8975056159918589 |
| h1-dist-to-r1 |  29.816004071755533 |
| h1-dist-to-h2 | -0.7691629173928523 |
| h2-dist-to-s1 |  1.7896567373964762 |
| h2-dist-to-r1 | -0.4967623936976533 |
| h2-dist-to-h1 | -0.7691629173928523 |
+---------------+---------------------+
------Cache for SimpleExtractor------
Cache has 755142 accesses and 3249154 additions.
 79000000000001610100004000100021110110001010100001111700000000000 had 81 references.
 790000000000011101000046000100021110110001010100001111700000000000 had 63 references.
 79000000000001160100004000100021110110001010100001111700000000000 had 68 references.
 79000000000006110100004000100021110110001010100001111700000000000 had 71 references.
 79000000000001110200001000100011110670001010100001111400000000000 had 84 references.
 79000000000001110200001000600011110170001010100001111400000000000 had 69 references.
 790000000000011102000010001000111101760001010100001111400000000000 had 27 references.
 79000000000001110200001000100011110170001010600001111400000000000 had 86 references.
 790000000000011102600001000100011110170001010100001111400000000000 had 55 references.
 790000000000071101000010001000126110110001010100001114100000000000 had 41 references.
 79000000000007110100006000100012110110001010100001114100000000000 had 13 references.
 79000000000007110100001000100012610110001010100001114100000000000 had 17 references.
 79000000000007110100001000100012110110006010100001114100000000000 had 41 references.
 79000000000007110100001000100062110110001010100001114100000000000 had 17 references.
 79000000000001110100001000100011110710004010200001161100000000000 had 58 references.
 79000000000001110100001000100011110710004010200006111100000000000 had 19 references.
 79000000000001110100001000100011110710004060200001111100000000000 had 38 references.
 79000000000001110100001000100011110710004010200001116100000000000 had 58 references.
 79000000000001110100001000100011110710004010200001611100000000000 had 36 references.
 79000000000001140100001000100011620110007010100001111100000000000 had 1 references.
(base) macintosh@BlckMagikMan StaghuntAgent % 
